{
  "meta": {
    "slug": "deploying-high-availability-postgresql-on-aks-with-streaming-replicas",
    "title": "Deploying High-Availability PostgreSQL on AKS with Streaming Replicas",
    "description": "A production-ready guide to running PostgreSQL on Azure Kubernetes Service using CloudNativePG &#8212; from zero to failover-tested cluster.",
    "date": "2026-01-31",
    "link": "https://shushankrecentendeavors.substack.com/p/deploying-high-availability-postgresql",
    "creator": "Shushank recent endeavors",
    "tags": [],
    "readTime": "15 min read",
    "coverImage": ""
  },
  "content": "<p>PostgreSQL now powers 36% of all database workloads running on Kubernetes — up 6 points since 2022. Running it on AKS with proper high availability isn’t just possible anymore; with the right operator, it’s actually <em>pleasant</em>. This guide walks through every step: provisioning the AKS infrastructure, deploying the CloudNativePG operator, configuring a primary with two streaming replicas across availability zones, setting up automated backups to Azure Blob Storage, and testing failover.</p><p>No hand-waving. Every YAML shown. Every <code>az</code> and <code>kubectl</code> command included.</p><div><hr></div><h2>Table of Contents</h2><ol><li><p><a href=\"#1-architecture-overview\">Architecture Overview</a></p></li><li><p><a href=\"#2-why-cloudnativepg\">Why CloudNativePG (and Not Patroni/Crunchy)</a></p></li><li><p><a href=\"#3-prerequisites\">Prerequisites</a></p></li><li><p><a href=\"#4-step-1--provision-aks-infrastructure\">Step 1 — Provision AKS Infrastructure</a></p></li><li><p><a href=\"#5-step-2--install-the-cloudnativepg-operator\">Step 2 — Install the CloudNativePG Operator</a></p></li><li><p><a href=\"#6-step-3--configure-storage\">Step 3 — Configure Storage</a></p></li><li><p><a href=\"#7-step-4--create-secrets\">Step 4 — Create Secrets</a></p></li><li><p><a href=\"#8-step-5--deploy-the-postgresql-cluster\">Step 5 — Deploy the PostgreSQL Cluster (1 Primary + 2 Replicas)</a></p></li><li><p><a href=\"#9-step-6--configure-backups\">Step 6 — Configure Backups to Azure Blob Storage</a></p></li><li><p><a href=\"#10-step-7--monitoring\">Step 7 — Set Up Monitoring (Prometheus + Grafana)</a></p></li><li><p><a href=\"#11-step-8--test-failover\">Step 8 — Test Failover</a></p></li><li><p><a href=\"#12-how-streaming-replication-works\">How Streaming Replication Actually Works</a></p></li><li><p><a href=\"#13-sync-vs-async\">Synchronous vs Asynchronous Replication</a></p></li><li><p><a href=\"#14-production-hardening\">Production Hardening Checklist</a></p></li><li><p><a href=\"#15-operator-comparison\">Operator Comparison: CNPG vs Crunchy PGO vs Zalando</a></p></li><li><p><a href=\"#16-common-pitfalls\">Common Pitfalls</a></p></li><li><p><a href=\"#17-conclusion\">Conclusion</a></p></li></ol><div><hr></div><h2>1. Architecture Overview</h2><p>Here’s what we’re building — a PostgreSQL cluster with one primary and two read replicas, each in a different Azure Availability Zone, managed by the CloudNativePG operator, with continuous backups to Azure Blob Storage:</p><pre><code><code>                        ┌─────────────────────────────────────────────────┐\n                        │              AKS Cluster (3 AZs)                │\n                        │                                                 │\n                        │  ┌──────────────────────────────────────────┐   │\n                        │  │        CloudNativePG Operator            │   │\n                        │  │    (watches Cluster CRD, manages HA)     │   │\n                        │  └──────────┬───────────┬───────────┬───────┘   │\n                        │             │           │           │           │\n                        │    ┌────────▼──┐  ┌─────▼─────┐  ┌─▼────────┐  │\n                        │    │  Primary   │  │ Replica 1 │  │ Replica 2│  │\n                        │    │ (Zone 1)   │  │ (Zone 2)  │  │ (Zone 3) │  │\n                        │    │            │  │           │  │          │  │\n                        │    │ pg-cluster │  │ pg-cluster│  │pg-cluster│  │\n                        │    │    -1      │  │    -2     │  │   -3     │  │\n                        │    │            │  │           │  │          │  │\n                        │    │  RW (5432) │  │  RO(5432) │  │ RO(5432) │  │\n                        │    └─────┬──────┘  └─────┬─────┘  └────┬─────┘  │\n                        │          │               │             │        │\n                        │          │  WAL Stream   │  WAL Stream │        │\n                        │          ├──────────────►│             │        │\n                        │          ├─────────────────────────────►        │\n                        │          │                                      │\n                        │    ┌─────▼──────────────────────────────────┐   │\n                        │    │         Azure Premium SSD v2           │   │\n                        │    │      (Persistent Volume Claims)        │   │\n                        │    └────────────────────────────────────────┘   │\n                        └──────────────────┬──────────────────────────────┘\n                                           │\n                                    WAL Archive\n                                           │\n                                  ┌────────▼────────┐\n                                  │  Azure Blob     │\n                                  │  Storage         │\n                                  │  (Backups +      │\n                                  │   WAL Archive)   │\n                                  └─────────────────┘\n</code></code></pre><h3>Three Kubernetes Services (auto-created by CNPG)</h3><pre><code><code>  ┌─────────────────────────────────────────────────────────┐\n  │                  Kubernetes Services                     │\n  │                                                         │\n  │  pg-cluster-rw ──────► Primary only (reads + writes)    │\n  │                                                         │\n  │  pg-cluster-ro ──────► Replicas only (read-only)        │\n  │                                                         │\n  │  pg-cluster-r  ──────► Any instance (reads from any)    │\n  └─────────────────────────────────────────────────────────┘\n</code></code></pre><p>Your application connects to <code>pg-cluster-rw</code> for writes and <code>pg-cluster-ro</code> for reads. If the primary dies, CNPG promotes a replica and re-points the <code>-rw</code> service. Zero application code changes needed.</p><div><hr></div><h2>2. Why CloudNativePG</h2><p>Three major PostgreSQL operators dominate the Kubernetes ecosystem. Here’s why this guide uses CloudNativePG:</p><p><strong>CloudNativePG (CNPG)</strong> — Built from scratch for Kubernetes. No external HA tools (no Patroni, no repmgr). Uses the Kubernetes API server directly as the source of truth. CNCF-hosted project backed by EDB and upstream PostgreSQL contributors. Microsoft’s official AKS documentation uses CNPG.</p><p><strong>Crunchy PGO</strong> — Battle-tested, built on Patroni + pgBackRest. More moving parts but arguably the most production-proven option. Uses Patroni as an intermediate HA layer between Kubernetes and PostgreSQL.</p><p><strong>Zalando postgres-operator</strong> — Also Patroni-based. Great for Zalando-style infrastructure but less actively maintained than the other two in recent years.</p><p>For AKS specifically, CNPG is the recommended path because Microsoft has partnered directly with the CloudNativePG project and all their official HA PostgreSQL documentation is built around it.</p><div><hr></div><h2>3. Prerequisites</h2><p>Before starting, you need:</p><ul><li><p><strong>Azure CLI</strong> (v2.60+) installed and authenticated (<code>az login</code>)</p></li><li><p><strong>kubectl</strong> installed (<code>az aks install-cli</code>)</p></li><li><p><strong>Helm</strong> v3 installed</p></li><li><p>An Azure subscription with <strong>Owner</strong> or <strong>User Access Administrator + Contributor</strong> roles</p></li><li><p>Basic familiarity with Kubernetes concepts (Pods, PVCs, Services, Namespaces)</p></li></ul><div><hr></div><h2>4. Step 1 — Provision AKS Infrastructure</h2><h3>Set environment variables</h3><pre><code><code># Customize these\nexport RESOURCE_GROUP=\"rg-postgres-ha\"\nexport CLUSTER_NAME=\"aks-postgres-ha\"\nexport LOCATION=\"eastus\"                    # Choose a region with 3 AZs\nexport K8S_VERSION=\"1.31\"                   # Check latest: az aks get-versions -l $LOCATION\nexport PG_NAMESPACE=\"postgres\"\nexport NODE_VM_SIZE=\"Standard_D4ds_v5\"      # 4 vCPU, 16 GB RAM — good starting point\n</code></code></pre><h3>Create the resource group</h3><pre><code><code>az group create \\\n  --name $RESOURCE_GROUP \\\n  --location $LOCATION\n</code></code></pre><h3>Create a multi-zone AKS cluster</h3><p>This is critical — we need nodes spread across 3 availability zones for true HA:</p><pre><code><code>az aks create \\\n  --resource-group $RESOURCE_GROUP \\\n  --name $CLUSTER_NAME \\\n  --node-count 3 \\\n  --zones 1 2 3 \\\n  --node-vm-size $NODE_VM_SIZE \\\n  --kubernetes-version $K8S_VERSION \\\n  --network-plugin azure \\\n  --network-plugin-mode overlay \\\n  --enable-managed-identity \\\n  --enable-workload-identity \\\n  --enable-oidc-issuer \\\n  --generate-ssh-keys \\\n  --tier standard\n</code></code></pre><p>Key flags explained:</p><ul><li><p><code>--zones 1 2 3</code> — Spreads nodes across all three Azure Availability Zones</p></li><li><p><code>--enable-workload-identity</code> — Required for CNPG to authenticate to Azure Blob Storage for backups without secrets</p></li><li><p><code>--enable-oidc-issuer</code> — Required for workload identity federation</p></li><li><p><code>--tier standard</code> — Production tier with uptime SLA (vs. the free tier)</p></li></ul><h3>Get credentials</h3><pre><code><code>az aks get-credentials \\\n  --resource-group $RESOURCE_GROUP \\\n  --name $CLUSTER_NAME\n</code></code></pre><h3>Verify nodes are spread across zones</h3><pre><code><code>kubectl get nodes -o custom-columns=\\\nNAME:.metadata.name,\\\nZONE:.metadata.labels.topology\\\\.kubernetes\\\\.io/zone\n</code></code></pre><p>Expected output — three nodes, three zones:</p><pre><code><code>NAME                              ZONE\naks-nodepool1-12345678-vmss0000   eastus-1\naks-nodepool1-12345678-vmss0001   eastus-2\naks-nodepool1-12345678-vmss0002   eastus-3\n</code></code></pre><div><hr></div><h2>5. Step 2 — Install the CloudNativePG Operator</h2><h3>Option A: Direct YAML manifest (simplest)</h3><pre><code><code>kubectl apply --server-side -f \\\n  https://raw.githubusercontent.com/cloudnative-pg/cloudnative-pg/release-1.25/releases/cnpg-1.25.1.yaml\n</code></code></pre><h3>Option B: Helm (recommended for production)</h3><pre><code><code>helm repo add cnpg https://cloudnative-pg.github.io/charts\nhelm repo update\n\nhelm upgrade --install cnpg \\\n  --namespace cnpg-system \\\n  --create-namespace \\\n  cnpg/cloudnative-pg\n</code></code></pre><h3>Verify the operator is running</h3><pre><code><code>kubectl get deployment -n cnpg-system\n</code></code></pre><pre><code><code>NAME                      READY   UP-TO-DATE   AVAILABLE\ncnpg-controller-manager   1/1     1            1\n</code></code></pre><h3>(Optional) Install the kubectl CNPG plugin</h3><p>This gives you <code>kubectl cnpg status</code>, <code>kubectl cnpg promote</code>, and other handy commands:</p><pre><code><code>curl -sSfL \\\n  https://github.com/cloudnative-pg/cloudnative-pg/raw/main/hack/install-cnpg-plugin.sh | \\\n  sudo sh -s -- -b /usr/local/bin\n</code></code></pre><div><hr></div><h2>6. Step 3 — Configure Storage</h2><p>PostgreSQL performance is tightly bound to storage I/O. On AKS, you have two good options:</p><pre><code><code>  ┌───────────────────────────────────────────────────────────┐\n  │                   Storage Options                         │\n  │                                                           │\n  │  ┌─────────────────────┐    ┌──────────────────────────┐  │\n  │  │   Premium SSD v2    │    │  Local NVMe + Azure      │  │\n  │  │   (managed-csi-     │    │  Container Storage       │  │\n  │  │    premium-ssd-v2)  │    │                          │  │\n  │  │                     │    │                          │  │\n  │  │  ✓ Zone-redundant   │    │  ✓ ~15,000 TPS           │  │\n  │  │  ✓ Configurable     │    │  ✓ Single-digit ms       │  │\n  │  │    IOPS/throughput   │    │    latency               │  │\n  │  │  ✓ Simpler setup    │    │  ✗ Ephemeral (need       │  │\n  │  │  ✓ Good for most    │    │    replicas for safety)  │  │\n  │  │    workloads         │    │  ✗ More complex setup    │  │\n  │  └─────────────────────┘    └──────────────────────────┘  │\n  └───────────────────────────────────────────────────────────┘\n</code></code></pre><p>For this guide, we’ll use <strong>Premium SSD v2</strong> — the simpler, more commonly used option. Create a StorageClass:</p><pre><code><code># storage-class.yaml\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: cnpg-premium-ssd\nprovisioner: disk.csi.azure.com\nparameters:\n  skuName: PremiumV2_LRS\n  DiskIOPSReadWrite: \"3000\"\n  DiskMBpsReadWrite: \"125\"\nallowVolumeExpansion: true\nreclaimPolicy: Delete\nvolumeBindingMode: WaitForFirstConsumer\n</code></code></pre><pre><code><code>kubectl apply -f storage-class.yaml\n</code></code></pre><p><code>WaitForFirstConsumer</code> is critical — it ensures the PVC is created in the same zone as the Pod, avoiding cross-zone data access.</p><div><hr></div><h2>7. Step 4 — Create Secrets</h2><h3>Application user password</h3><pre><code><code>kubectl create namespace $PG_NAMESPACE\n\n# Generate a strong random password\nPG_APP_PASSWORD=$(openssl rand -base64 24)\n\nkubectl create secret generic pg-app-user \\\n  --from-literal=username=app \\\n  --from-literal=password=\"${PG_APP_PASSWORD}\" \\\n  --namespace $PG_NAMESPACE\n\necho \"App password: $PG_APP_PASSWORD\"\n# Save this somewhere safe!\n</code></code></pre><h3>Superuser password (optional — CNPG auto-generates one if omitted)</h3><pre><code><code>PG_SUPERUSER_PASSWORD=$(openssl rand -base64 24)\n\nkubectl create secret generic pg-superuser \\\n  --from-literal=username=postgres \\\n  --from-literal=password=\"${PG_SUPERUSER_PASSWORD}\" \\\n  --namespace $PG_NAMESPACE\n</code></code></pre><div><hr></div><h2>8. Step 5 — Deploy the PostgreSQL Cluster</h2><p>This is the main event. One YAML creates a primary and two streaming replicas:</p><pre><code><code># pg-cluster.yaml\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  name: pg-cluster\n  namespace: postgres\nspec:\n  # -------------------------------------------------------\n  # INSTANCES: 1 primary + 2 replicas = 3 total\n  # -------------------------------------------------------\n  instances: 3\n  imageName: ghcr.io/cloudnative-pg/postgresql:17.2\n\n  # -------------------------------------------------------\n  # PRIMARY UPDATE STRATEGY\n  # -------------------------------------------------------\n  # \"unsupervised\" = automatic switchover during rolling updates\n  # \"supervised\"   = wait for manual switchover (safer for prod)\n  primaryUpdateStrategy: unsupervised\n\n  # -------------------------------------------------------\n  # STORAGE\n  # -------------------------------------------------------\n  storage:\n    storageClass: cnpg-premium-ssd\n    size: 50Gi\n\n  walStorage:\n    storageClass: cnpg-premium-ssd\n    size: 10Gi\n\n  # -------------------------------------------------------\n  # BOOTSTRAP — Initialize a new cluster\n  # -------------------------------------------------------\n  bootstrap:\n    initdb:\n      database: appdb\n      owner: app\n      secret:\n        name: pg-app-user\n      postInitSQL:\n        - CREATE EXTENSION IF NOT EXISTS pg_stat_statements;\n        - CREATE EXTENSION IF NOT EXISTS pgcrypto;\n\n  # -------------------------------------------------------\n  # SUPERUSER SECRET\n  # -------------------------------------------------------\n  superuserSecret:\n    name: pg-superuser\n\n  # -------------------------------------------------------\n  # POSTGRESQL CONFIGURATION\n  # -------------------------------------------------------\n  postgresql:\n    parameters:\n      # Memory (tune based on your node size)\n      shared_buffers: \"1GB\"\n      effective_cache_size: \"3GB\"\n      work_mem: \"64MB\"\n      maintenance_work_mem: \"256MB\"\n\n      # WAL\n      wal_level: \"replica\"\n      max_wal_senders: \"10\"\n      max_replication_slots: \"10\"\n      wal_buffers: \"64MB\"\n\n      # Checkpoints\n      checkpoint_completion_target: \"0.9\"\n      checkpoint_timeout: \"15min\"\n\n      # Query planner\n      random_page_cost: \"1.1\"       # SSD-optimized\n      effective_io_concurrency: \"200\"\n\n      # Logging\n      log_min_duration_statement: \"1000\"   # Log queries &gt; 1s\n      log_checkpoints: \"on\"\n      log_connections: \"on\"\n      log_disconnections: \"on\"\n\n    pg_hba:\n      - host all all 10.0.0.0/8 scram-sha-256\n      - host all all 172.16.0.0/12 scram-sha-256\n\n  # -------------------------------------------------------\n  # ANTI-AFFINITY — One instance per zone\n  # -------------------------------------------------------\n  affinity:\n    enablePodAntiAffinity: true\n    topologyKey: topology.kubernetes.io/zone\n    podAntiAffinityType: required\n\n  # -------------------------------------------------------\n  # RESOURCES\n  # -------------------------------------------------------\n  resources:\n    requests:\n      memory: \"2Gi\"\n      cpu: \"1\"\n    limits:\n      memory: \"4Gi\"\n      cpu: \"2\"\n\n  # -------------------------------------------------------\n  # MONITORING\n  # -------------------------------------------------------\n  monitoring:\n    enablePodMonitor: true\n\n  # -------------------------------------------------------\n  # MAINTENANCE WINDOWS\n  # -------------------------------------------------------\n  nodeMaintenanceWindow:\n    inProgress: false\n    reusePVC: true\n</code></code></pre><h3>Deploy it</h3><pre><code><code>kubectl apply -f pg-cluster.yaml\n</code></code></pre><h3>Watch it come up</h3><pre><code><code>kubectl get pods -n postgres -w\n</code></code></pre><pre><code><code>NAME            READY   STATUS    RESTARTS   AGE\npg-cluster-1    1/1     Running   0          2m     # Primary\npg-cluster-2    1/1     Running   0          90s    # Replica 1\npg-cluster-3    1/1     Running   0          60s    # Replica 2\n</code></code></pre><h3>Check cluster status</h3><pre><code><code>kubectl cnpg status pg-cluster -n postgres\n</code></code></pre><pre><code><code>Cluster Summary\nName:              pg-cluster\nNamespace:         postgres\nPostgreSQL Image:  ghcr.io/cloudnative-pg/postgresql:17.2\nPrimary instance:  pg-cluster-1\nStatus:            Cluster in healthy state\nInstances:         3\nReady instances:   3\n\nInstances status\nName           Database Size  Current LSN  Replication role  Status\n----           -------------  -----------  ----------------  ------\npg-cluster-1   42 MB          0/7000060    Primary           OK\npg-cluster-2   42 MB          0/7000060    Standby (async)   OK\npg-cluster-3   42 MB          0/7000060    Standby (async)   OK\n</code></code></pre><h3>Verify zone distribution</h3><pre><code><code>kubectl get pods -n postgres -o custom-columns=\\\nPOD:.metadata.name,\\\nNODE:.spec.nodeName,\\\nZONE:.metadata.labels.topology\\\\.kubernetes\\\\.io/zone\n</code></code></pre><pre><code><code>POD             NODE                              ZONE\npg-cluster-1    aks-nodepool1-...-vmss0000        eastus-1\npg-cluster-2    aks-nodepool1-...-vmss0001        eastus-2\npg-cluster-3    aks-nodepool1-...-vmss0002        eastus-3\n</code></code></pre><div><hr></div><h2>9. Step 6 — Configure Backups</h2><h3>Create a storage account for backups</h3><pre><code><code>STORAGE_ACCOUNT_NAME=\"pgbackups$(openssl rand -hex 4)\"\n\naz storage account create \\\n  --name $STORAGE_ACCOUNT_NAME \\\n  --resource-group $RESOURCE_GROUP \\\n  --location $LOCATION \\\n  --sku Standard_ZRS \\\n  --kind StorageV2\n\naz storage container create \\\n  --name backups \\\n  --account-name $STORAGE_ACCOUNT_NAME\n</code></code></pre><h3>Set up Workload Identity for backup access (no secrets!)</h3><pre><code><code># Get the OIDC issuer URL\nAKS_OIDC_ISSUER=$(az aks show \\\n  --name $CLUSTER_NAME \\\n  --resource-group $RESOURCE_GROUP \\\n  --query \"oidcIssuerProfile.issuerUrl\" -o tsv)\n\n# Create a managed identity\naz identity create \\\n  --name mi-pg-backup \\\n  --resource-group $RESOURCE_GROUP \\\n  --location $LOCATION\n\nMI_CLIENT_ID=$(az identity show \\\n  --name mi-pg-backup \\\n  --resource-group $RESOURCE_GROUP \\\n  --query clientId -o tsv)\n\nMI_OBJECT_ID=$(az identity show \\\n  --name mi-pg-backup \\\n  --resource-group $RESOURCE_GROUP \\\n  --query principalId -o tsv)\n\n# Grant Storage Blob Data Contributor role\nSTORAGE_ACCOUNT_ID=$(az storage account show \\\n  --name $STORAGE_ACCOUNT_NAME \\\n  --resource-group $RESOURCE_GROUP \\\n  --query id -o tsv)\n\naz role assignment create \\\n  --assignee-object-id $MI_OBJECT_ID \\\n  --role \"Storage Blob Data Contributor\" \\\n  --scope $STORAGE_ACCOUNT_ID\n\n# Create federated credential\n# CNPG creates a service account named after the cluster\naz identity federated-credential create \\\n  --name pg-cluster-fedcred \\\n  --identity-name mi-pg-backup \\\n  --resource-group $RESOURCE_GROUP \\\n  --issuer $AKS_OIDC_ISSUER \\\n  --subject system:serviceaccount:${PG_NAMESPACE}:pg-cluster \\\n  --audience api://AzureADTokenExchange\n</code></code></pre><h3>Add backup configuration to the cluster</h3><p>Update <code>pg-cluster.yaml</code> — add these sections under <code>spec:</code>:</p><pre><code><code>  # -------------------------------------------------------\n  # SERVICE ACCOUNT ANNOTATION (Workload Identity)\n  # -------------------------------------------------------\n  serviceAccountTemplate:\n    metadata:\n      annotations:\n        azure.workload.identity/client-id: \"&lt;MI_CLIENT_ID&gt;\"\n      labels:\n        azure.workload.identity/use: \"true\"\n\n  # -------------------------------------------------------\n  # BACKUP CONFIGURATION\n  # -------------------------------------------------------\n  backup:\n    barmanObjectStore:\n      destinationPath: \"https://&lt;STORAGE_ACCOUNT_NAME&gt;.blob.core.windows.net/backups/\"\n      azureCredentials:\n        inheritFromAzureAD: true\n      wal:\n        compression: gzip\n        maxParallel: 2\n      data:\n        compression: gzip\n    retentionPolicy: \"30d\"\n</code></code></pre><p>Replace <code>&lt;MI_CLIENT_ID&gt;</code> and <code>&lt;STORAGE_ACCOUNT_NAME&gt;</code> with your actual values.</p><pre><code><code>kubectl apply -f pg-cluster.yaml\n</code></code></pre><h3>Create a scheduled backup</h3><pre><code><code># scheduled-backup.yaml\napiVersion: postgresql.cnpg.io/v1\nkind: ScheduledBackup\nmetadata:\n  name: pg-cluster-daily-backup\n  namespace: postgres\nspec:\n  schedule: \"0 2 * * *\"          # 2 AM UTC daily\n  backupOwnerReference: self\n  cluster:\n    name: pg-cluster\n  method: barmanObjectStore\n  immediate: true                 # Take first backup immediately\n</code></code></pre><pre><code><code>kubectl apply -f scheduled-backup.yaml\n</code></code></pre><h3>Verify backup</h3><pre><code><code>kubectl get backups -n postgres\n</code></code></pre><pre><code><code>NAME                             AGE   COMPLETED   ...\npg-cluster-daily-backup-20260201   1m    True\n</code></code></pre><div><hr></div><h2>10. Step 7 — Monitoring</h2><h3>Deploy Prometheus + Grafana with Helm</h3><pre><code><code>helm repo add prometheus-community \\\n  https://prometheus-community.github.io/helm-charts\nhelm repo update\n\nhelm upgrade --install prometheus \\\n  prometheus-community/kube-prometheus-stack \\\n  --namespace monitoring \\\n  --create-namespace \\\n  --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false \\\n  --set prometheus.prometheusSpec.podMonitorSelectorNilUsesHelmValues=false\n</code></code></pre><h3>Create a PodMonitor for CNPG</h3><pre><code><code># pg-pod-monitor.yaml\napiVersion: monitoring.coreos.com/v1\nkind: PodMonitor\nmetadata:\n  name: pg-cluster-monitor\n  namespace: postgres\n  labels:\n    cnpg.io/cluster: pg-cluster\nspec:\n  selector:\n    matchLabels:\n      cnpg.io/cluster: pg-cluster\n  podMetricsEndpoints:\n    - port: metrics\n</code></code></pre><pre><code><code>kubectl apply -f pg-pod-monitor.yaml\n</code></code></pre><p>Each CNPG instance exposes metrics at <code>:9187/metrics</code> including replication lag, transaction rates, connection counts, WAL generation rate, and buffer hit ratios. CloudNativePG provides a ready-made Grafana dashboard you can import.</p><div><hr></div><h2>11. Step 8 — Test Failover</h2><p>This is the most important step. Never ship to production without testing failover.</p><h3>Check current state</h3><pre><code><code>kubectl cnpg status pg-cluster -n postgres\n</code></code></pre><p>Note which pod is the primary (e.g., <code>pg-cluster-1</code>).</p><h3>Simulate a failure — delete the primary pod</h3><pre><code><code>kubectl delete pod pg-cluster-1 -n postgres\n</code></code></pre><h3>Watch the failover</h3><pre><code><code>kubectl get pods -n postgres -w\n</code></code></pre><p>What happens:</p><pre><code><code>  BEFORE FAILOVER                         AFTER FAILOVER\n  ================                        ================\n\n  pg-cluster-1  ◄── Primary (DELETED)     pg-cluster-1  ◄── Replica (rebuilt)\n  pg-cluster-2  ◄── Replica               pg-cluster-2  ◄── PRIMARY (promoted!)\n  pg-cluster-3  ◄── Replica               pg-cluster-3  ◄── Replica\n\n  Failover time: typically 5-30 seconds\n</code></code></pre><h3>Verify the new topology</h3><pre><code><code>kubectl cnpg status pg-cluster -n postgres\n</code></code></pre><p>The <code>-rw</code> service automatically re-points to the new primary. Applications connected via the service DNS name reconnect transparently.</p><h3>Controlled switchover (for maintenance)</h3><pre><code><code>kubectl cnpg promote pg-cluster pg-cluster-3 -n postgres\n</code></code></pre><p>This does a graceful switchover — completes in-flight transactions on the old primary, promotes the target replica, then demotes the old primary to replica. Much cleaner than a crash failover.</p><div><hr></div><h2>12. How Streaming Replication Actually Works</h2><p>Understanding what’s happening under the hood helps you debug replication lag and make informed tuning decisions.</p><pre><code><code>  PRIMARY                                    REPLICA\n  =======                                    =======\n\n  Client ──► BEGIN; INSERT ...; COMMIT;\n                │\n                ▼\n         ┌─────────────┐\n         │  WAL Buffer  │  ◄── Transaction written to WAL first\n         │  (in memory) │      (Write-Ahead Logging)\n         └──────┬───────┘\n                │\n         ┌──────▼───────┐\n         │  WAL Segment  │  ◄── Flushed to disk (pg_wal/)\n         │  Files (16MB  │      Each segment = 16 MB\n         │  each)        │\n         └──────┬───────┘\n                │\n       wal_sender process ──────────────────► wal_receiver process\n       (on primary)           TCP/TLS          (on replica)\n                              connection\n                                                     │\n                                              ┌──────▼───────┐\n                                              │  WAL applied  │\n                                              │  to replica   │\n                                              │  data files   │\n                                              └──────────────┘\n                                                     │\n                                              Replica is now\n                                              consistent with\n                                              primary (modulo lag)\n</code></code></pre><p>Key processes involved:</p><ul><li><p><strong>wal_sender</strong> — Runs on the primary. One process per connected replica. Reads WAL records and sends them over the network.</p></li><li><p><strong>wal_receiver</strong> — Runs on each replica. Connects to the primary, receives WAL records, writes them to local WAL files.</p></li><li><p><strong>startup process</strong> — Runs on each replica. Replays received WAL records against the local data files.</p></li></ul><h3>Checking replication status</h3><pre><code><code># Connect to the primary\nkubectl exec -it pg-cluster-1 -n postgres -- psql -U postgres\n\n# Check replication status\nSELECT client_addr,\n       state,\n       sent_lsn,\n       write_lsn,\n       flush_lsn,\n       replay_lsn,\n       write_lag,\n       flush_lag,\n       replay_lag\nFROM pg_stat_replication;\n</code></code></pre><div><hr></div><h2>13. Synchronous vs Asynchronous Replication</h2><p>By default, CNPG uses <strong>asynchronous replication</strong> — the primary doesn’t wait for replicas to confirm receipt before committing. This is faster but has a small window for data loss during failover.</p><pre><code><code>  ASYNCHRONOUS (default)               SYNCHRONOUS\n  =====================                ===========\n\n  Client: COMMIT                       Client: COMMIT\n     │                                    │\n     ▼                                    ▼\n  Primary: Write WAL                   Primary: Write WAL\n     │                                    │\n     ▼                                    ├──► Send to replica\n  Return \"OK\" to client               Wait for replica ACK...\n     │                                    │        │\n     └──► Send WAL to replicas            │   ◄────┘ (ACK received)\n           (async, fire-and-forget)       ▼\n                                       Return \"OK\" to client\n\n  ✓ Lower latency                      ✓ Zero data loss (RPO=0)\n  ✗ Potential data loss on             ✗ Higher write latency\n    failover (RPO &gt; 0)                 ✗ Writes blocked if replica\n                                         is unavailable\n</code></code></pre><h3>Enable synchronous replication</h3><p>Add to your Cluster spec:</p><pre><code><code>spec:\n  minSyncReplicas: 1\n  maxSyncReplicas: 2\n</code></code></pre><p>With <code>minSyncReplicas: 1</code>, at least one replica must confirm every transaction before the primary returns success. This gives you <strong>RPO = 0</strong> (zero data loss) at the cost of higher write latency.</p><p>CNPG automatically manages the <code>synchronous_standby_names</code> PostgreSQL parameter based on these values.</p><h3>Self-healing mode</h3><p>If you want synchronous replication that degrades gracefully when replicas are unavailable (writes continue even if no sync replica is up):</p><pre><code><code>spec:\n  minSyncReplicas: 1\n  maxSyncReplicas: 2\n  postgresql:\n    parameters:\n      cnpg.io/dataDurability: \"preferred\"    # vs \"required\" (default)\n</code></code></pre><div><hr></div><h2>14. Production Hardening Checklist</h2><pre><code><code>  ┌─────────────────────────────────────────────────────────────┐\n  │              PRODUCTION HARDENING                            │\n  │                                                             │\n  │  ☐  Multi-zone AKS (--zones 1 2 3)                         │\n  │  ☐  Pod anti-affinity by zone (topology.kubernetes.io/zone) │\n  │  ☐  Separate WAL storage (walStorage in Cluster spec)       │\n  │  ☐  Premium SSD v2 or NVMe storage                         │\n  │  ☐  Resource requests AND limits set                        │\n  │  ☐  Daily automated backups to Blob Storage                 │\n  │  ☐  WAL archiving enabled (continuous PITR)                 │\n  │  ☐  Monitoring (Prometheus + PodMonitor)                    │\n  │  ☐  Connection pooling (PgBouncer via CNPG Pooler CRD)     │\n  │  ☐  TLS enabled (on by default in CNPG)                    │\n  │  ☐  Network policies limiting database access               │\n  │  ☐  PDB (PodDisruptionBudget) — CNPG creates one           │\n  │  ☐  Tested failover in staging                              │\n  │  ☐  Tested backup restore in staging                        │\n  │  ☐  Log queries &gt; 1s (log_min_duration_statement)           │\n  │  ☐  pg_stat_statements enabled                              │\n  │  ☐  Alerting on replication lag &gt; threshold                 │\n  │  ☐  Alerting on backup failures                             │\n  │  ☐  Alerting on disk usage &gt; 80%                            │\n  └─────────────────────────────────────────────────────────────┘\n</code></code></pre><h3>Add PgBouncer connection pooling</h3><pre><code><code># pg-pooler.yaml\napiVersion: postgresql.cnpg.io/v1\nkind: Pooler\nmetadata:\n  name: pg-cluster-pooler-rw\n  namespace: postgres\nspec:\n  cluster:\n    name: pg-cluster\n  instances: 2\n  type: rw\n  pgbouncer:\n    poolMode: transaction\n    parameters:\n      max_client_conn: \"1000\"\n      default_pool_size: \"25\"\n</code></code></pre><pre><code><code>kubectl apply -f pg-pooler.yaml\n</code></code></pre><p>Applications connect to the Pooler service instead of the cluster service directly.</p><h3>Add a Network Policy</h3><pre><code><code># pg-network-policy.yaml\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: pg-cluster-access\n  namespace: postgres\nspec:\n  podSelector:\n    matchLabels:\n      cnpg.io/cluster: pg-cluster\n  policyTypes:\n    - Ingress\n  ingress:\n    - from:\n        - namespaceSelector:\n            matchLabels:\n              app.kubernetes.io/part-of: my-application\n      ports:\n        - protocol: TCP\n          port: 5432\n</code></code></pre><div><hr></div><h2>15. Operator Comparison</h2><p>FeatureCloudNativePGCrunchy PGOZalando<strong>HA Mechanism</strong>Kubernetes-native (no Patroni)PatroniPatroni<strong>Backup Tool</strong>Barman CloudpgBackRestWAL-G / WAL-E<strong>Connection Pooler</strong>PgBouncer (built-in CRD)PgBouncer (built-in)External<strong>Monitoring</strong>Prometheus exporter (built-in)pgMonitor + PrometheusSeparate setup<strong>License</strong>Apache 2.0Apache 2.0MIT<strong>AKS Official Support</strong>✓ (Microsoft docs)CommunityCommunity<strong>Major Version Upgrades</strong>Via importVia PGO (pg_upgrade)Manual<strong>CNCF Project</strong>✓✗✗<strong>Kubernetes-Only Dependencies</strong>✓✗ (etcd via Patroni)✗ (etcd via Patroni)<strong>Maturity</strong>Since 2022 (EDB roots)Since 2017Since 2017</p><div><hr></div><h2>16. Common Pitfalls</h2><h3>1. Storage in the wrong zone</h3><p>If <code>volumeBindingMode</code> isn’t set to <code>WaitForFirstConsumer</code>, your PVC might be provisioned in Zone 1 while the Pod runs in Zone 2. This either fails entirely or adds catastrophic latency. Always use <code>WaitForFirstConsumer</code>.</p><h3>2. Forgetting WAL storage separation</h3><p>PostgreSQL writes WAL sequentially and data files randomly. Putting them on the same disk means they compete for I/O. Use <code>walStorage</code> in the Cluster spec to give WAL its own PVC.</p><h3>3. Not testing backup restores</h3><p>A backup that hasn’t been tested is not a backup. Periodically restore from your Blob Storage backup into a test cluster:</p><pre><code><code>apiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  name: pg-restore-test\n  namespace: postgres-test\nspec:\n  instances: 1\n  storage:\n    size: 50Gi\n    storageClass: cnpg-premium-ssd\n  bootstrap:\n    recovery:\n      source: pg-cluster-backup\n  externalClusters:\n    - name: pg-cluster-backup\n      barmanObjectStore:\n        destinationPath: \"https://&lt;STORAGE&gt;.blob.core.windows.net/backups/\"\n        azureCredentials:\n          inheritFromAzureAD: true\n</code></code></pre><h3>4. Ignoring replication lag</h3><p>Even with async replication, lag should be measured in milliseconds. If it’s seconds, something is wrong — usually storage bottleneck, network issues, or the replica is under-provisioned. Set alerts on <code>replay_lag &gt; 1s</code>.</p><h3>5. Running a single instance and calling it “HA”</h3><p><code>instances: 1</code> means no replicas. If that pod dies, you’re down until the PVC reattaches (could be minutes). Minimum for HA is <code>instances: 3</code> across 3 zones.</p><h3>6. Using ALTER SYSTEM</h3><p>CNPG disables <code>ALTER SYSTEM</code> by default because changes made via <code>ALTER SYSTEM</code> aren’t replicated to standbys and aren’t managed by the operator. All configuration must go through the Cluster CRD YAML.</p><div><hr></div><h2>17. Conclusion</h2><p>Here’s the complete flow of what we built:</p><pre><code><code>  ┌──────────────────────────────────────────────────────────────┐\n  │                        WHAT WE BUILT                         │\n  │                                                              │\n  │  AKS Cluster (3 AZs, Standard tier)                         │\n  │    │                                                         │\n  │    ├── CloudNativePG Operator                                │\n  │    │     │                                                   │\n  │    │     └── PostgreSQL Cluster                              │\n  │    │           ├── Primary    (Zone 1, Premium SSD v2)       │\n  │    │           ├── Replica 1  (Zone 2, streaming async)      │\n  │    │           └── Replica 2  (Zone 3, streaming async)      │\n  │    │                                                         │\n  │    ├── Services                                              │\n  │    │     ├── pg-cluster-rw  → Primary (reads + writes)       │\n  │    │     ├── pg-cluster-ro  → Replicas (read-only)           │\n  │    │     └── pg-cluster-r   → Any instance                   │\n  │    │                                                         │\n  │    ├── PgBouncer Pooler (connection pooling)                 │\n  │    │                                                         │\n  │    ├── Automated Backups → Azure Blob Storage                │\n  │    │     └── Daily full + continuous WAL archiving            │\n  │    │                                                         │\n  │    └── Monitoring                                            │\n  │          └── Prometheus + Grafana (PodMonitor)               │\n  │                                                              │\n  │  FAILOVER: Automatic, 5-30 seconds, zero app code changes   │\n  │  RPO:      ~0 (async) or exactly 0 (sync mode)              │\n  │  RTO:      &lt; 30 seconds                                     │\n  │  BACKUPS:  PITR to any point in the last 30 days             │\n  └──────────────────────────────────────────────────────────────┘\n</code></code></pre><p>The combination of AKS multi-zone clusters, CloudNativePG’s operator, and PostgreSQL’s native streaming replication gives you a genuinely production-grade database platform. The operator handles the hard parts — failover detection, replica promotion, service re-routing, WAL management, backup scheduling — while you get to define everything declaratively in version-controlled YAML.</p><p>Start with async replication and 3 instances across 3 zones. Monitor replication lag. Test failover monthly. Test backup restores monthly. That’s 90% of what separates a production database from a resume-driven one.</p><div><hr></div><p><em>Built with CloudNativePG 1.25, PostgreSQL 17, AKS 1.31, Azure Premium SSD v2.</em></p>"
}